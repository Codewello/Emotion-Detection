{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_bBZvHfLcDf"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Bidirectional\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from keras.layers import *\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYdSbH6cLh3z"
      },
      "outputs": [],
      "source": [
        "# import the data frame\n",
        "# df=pd.read_csv()\n",
        "# show the first 5 rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W_FYN3tLvBd"
      },
      "outputs": [],
      "source": [
        "# clean the data frame if needed\n",
        "# drop the null values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXl94xUVPo1j"
      },
      "source": [
        "**Tokenization**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl-VG2U2Pg8P"
      },
      "outputs": [],
      "source": [
        "# tokenization for each word you have one the texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0TTLewuQG4T"
      },
      "source": [
        "**Padding**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRbL9OCFQIR_"
      },
      "outputs": [],
      "source": [
        "def padd(arr):\n",
        "    for i in range(100-len(arr)):\n",
        "        arr.append('<pad>')\n",
        "    return arr[:100]\n",
        "\n",
        "# call the padd function for each sentence in your tokenization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDnXnoESQj4a"
      },
      "source": [
        "**Embedding**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9qTyre9Qun9"
      },
      "outputs": [],
      "source": [
        "# vocab_f ='/glove.6B.50d.txt'\n",
        "# download it from here :https://www.kaggle.com/datasets/watts2/glove6b50dtxt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFUsekFaQbRU"
      },
      "outputs": [],
      "source": [
        "embeddings_index = {}\n",
        "with open(vocab_f, encoding='utf8') as f:\n",
        "    for line in f:\n",
        "\n",
        "        values = line.rstrip().rsplit(' ')\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqbdNyzRQ8nr"
      },
      "outputs": [],
      "source": [
        "# Embedding each word of the emotion_arr\n",
        "# embedded_emotion_arr = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZCNQZJyRDq5"
      },
      "source": [
        "**One Hot Encoding**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq6UJO60RDLM"
      },
      "outputs": [],
      "source": [
        "# Perform one-hot encoding from sklearn\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGJ3mMzZRMaA"
      },
      "outputs": [],
      "source": [
        "# Split into train and test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCKEX_poRRQp"
      },
      "source": [
        "**Create the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrE2sbuNRTXc"
      },
      "outputs": [],
      "source": [
        "# Defining the BiLSTM Model\n",
        "def model(X, Y, input_size1, input_size2, output_size):\n",
        "    m = Sequential()  # Create a Sequential model\n",
        "\n",
        "    # Add a Bidirectional LSTM layer with 100 units\n",
        "    # Add a Dense (fully connected) layer with 'output_size' units and softmax activation\n",
        "    # Compile the model with 'Adam' optimizer, 'categorical_crossentropy' loss, and 'accuracy' metric\n",
        "    # Train the model on input data 'X' and target labels 'Y' for 32 epochs, using batches of size 128\n",
        "\n",
        "    return m  # Return the compiled and trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtJeTK9LRaMN"
      },
      "outputs": [],
      "source": [
        "# train the model\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
